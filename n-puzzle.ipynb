{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Copyright **`(c)`** 2024 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free under certain conditions â€” see the [`license`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from random import choice\n",
    "import numpy as np\n",
    "import heapq\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random but solvable puzzle configuration 5x5:\n",
      "[[ 7  1  0  4  5]\n",
      " [ 2 11  3 13 10]\n",
      " [ 6  8  9 24 14]\n",
      " [16 12 17 18 15]\n",
      " [21 22 23 20 19]]\n"
     ]
    }
   ],
   "source": [
    "# Auxiliary functions\n",
    "def available_actions(state: np.ndarray) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Calculate the available moves given the state.\n",
    "    \"\"\"\n",
    "    # np.where() returns a tuple in the format ([row_index], [column_index])\n",
    "    row, col = np.where(state == 0)  # Find the position of the 0\n",
    "    actions = []\n",
    "    if row > 0:  # I can move the 0 up\n",
    "        actions.append((-1, 0))\n",
    "    if row < state.shape[0] - 1:  # I can move the 0 down\n",
    "        actions.append((1, 0))\n",
    "    if col > 0:  # I can move the 0 to the left\n",
    "        actions.append((0, -1))\n",
    "    if col < state.shape[1] - 1:  # I can move the 0 to the right\n",
    "        actions.append((0, 1))\n",
    "    return actions\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies an action to the state.\n",
    "    \"\"\"\n",
    "    new_state = state.copy()\n",
    "    row, col = np.where(state == 0)  # Find the position of the 0\n",
    "    row, col = row[0], col[0]\n",
    "    # dr is the offset to add to the row index to get the row index of the new empty tile\n",
    "    # dc is the offset to add to the column index to get the column index of the new empty tile\n",
    "    dr, dc = action\n",
    "    new_row, new_col = row + dr, col + dc\n",
    "    # Swap the zero with the value at the new position calculated in the previous line of code\n",
    "    new_state[row, col], new_state[new_row, new_col] = new_state[new_row, new_col], new_state[row, col]\n",
    "    return new_state\n",
    "\n",
    "\n",
    "def generate_random_solvable_puzzle(goal_state: np.ndarray, steps: int = 100000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates a random but solvable configuration of the puzzle starting from the goal state.\n",
    "    \n",
    "    Parameters:\n",
    "    - goal_state: np.ndarray, the goal state of the puzzle.\n",
    "    - steps: int, the number of random moves to apply to generate the initial state.\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray, the generated random state.\n",
    "\n",
    "    \"\"\"\n",
    "    current_state = goal_state.copy()\n",
    "    for _ in range(steps):\n",
    "        action = choice(available_actions(current_state))\n",
    "        current_state = do_action(current_state, action)\n",
    "    return current_state\n",
    "\n",
    "\n",
    "PUZZLE_DIM = 5\n",
    "goal_state = np.array([i for i in range(1, PUZZLE_DIM**2)] + [0]).reshape((PUZZLE_DIM, PUZZLE_DIM))\n",
    "\n",
    "# Generate a random but solvable configuration\n",
    "random_solvable = generate_random_solvable_puzzle(goal_state, steps=100)\n",
    "state = random_solvable\n",
    "\n",
    "print(f\"Random but solvable puzzle configuration {PUZZLE_DIM}x{PUZZLE_DIM}:\")\n",
    "print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = namedtuple('Action', ['pos1', 'pos2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def available_actions(state: np.ndarray) -> list['Action']:\n",
    "    x, y = [int(_[0]) for _ in np.where(state == 0)]\n",
    "    actions = list()\n",
    "    if x > 0:\n",
    "        actions.append(action((x, y), (x - 1, y)))\n",
    "    if x < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x + 1, y)))\n",
    "    if y > 0:\n",
    "        actions.append(action((x, y), (x, y - 1)))\n",
    "    if y < PUZZLE_DIM - 1:\n",
    "        actions.append(action((x, y), (x, y + 1)))\n",
    "    return actions\n",
    "\n",
    "\n",
    "\n",
    "def do_action(state: np.ndarray, action: 'Action') -> np.ndarray:\n",
    "    new_state = state.copy()\n",
    "    new_state[action.pos1], new_state[action.pos2] = new_state[action.pos2], new_state[action.pos1]\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of moves to solve the puzzle: 25\n",
      "Number of explored nodes: 27\n",
      "Execution time: 0:00:00.007607\n",
      "Starting state:\n",
      "[[ 7  1  3  4  5]\n",
      " [ 2 11  0 13 10]\n",
      " [ 6  8  9 24 14]\n",
      " [16 12 17 18 15]\n",
      " [21 22 23 20 19]]\n",
      "\n",
      "Goal state:\n",
      "[[ 1  2  3  4  5]\n",
      " [ 6  7  8  9 10]\n",
      " [11 12 13 14 15]\n",
      " [16 17 18 19 20]\n",
      " [21 22 23 24  0]]\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate the Manhattan distance\n",
    "def manhattan_distance(state: np.ndarray, goal_state: np.ndarray) -> int:\n",
    "    distance = 0\n",
    "    for x in range(state.shape[0]):\n",
    "        for y in range(state.shape[1]):\n",
    "            if state[x, y] != 0:  # Ignore the empty tile (0)\n",
    "                goal_x, goal_y = np.where(goal_state == state[x, y])\n",
    "                distance += abs(goal_x[0] - x) + abs(goal_y[0] - y)\n",
    "    return distance\n",
    "\n",
    "\n",
    "# Implementation of the A* algorithm with Graph Search\n",
    "def a_star_search_with_metrics(start_state: np.ndarray, goal_state: np.ndarray):\n",
    "\n",
    "    frontier = []  # Priority queue for the frontier\n",
    "    heapq.heappush(frontier, (0, start_state.tobytes(), start_state))\n",
    "    \n",
    "    explored: Set[bytes] = set()  # States already examined\n",
    "    # The came_from dictionary is used to track the optimal path to the goal node in a search algorithm like A*. \n",
    "    # It is structured so that each state (represented as a key) points to its parent state and the action that led to the transition.\n",
    "    # came_from = {\n",
    "    #   state: (parent_state, action)\n",
    "    # }\n",
    "    came_from: Dict[bytes, Tuple[np.ndarray, 'Action']] = {}  # Reconstructs the path\n",
    "\n",
    "    g_costs = {start_state.tobytes(): 0}  # Accumulated costs for each state\n",
    "    nodes_explored = 0  # Counter for examined/expanded nodes\n",
    "    \n",
    "    while frontier:\n",
    "        _, current_bytes, current_state = heapq.heappop(frontier)\n",
    "        \n",
    "        # This check is needed because I might have added the same state to the frontier more than once\n",
    "        # and I want to ensure that, once a state has been examined/expanded, it is not processed a second time.\n",
    "        if current_bytes in explored:\n",
    "            continue\n",
    "        \n",
    "        # Increment the counter of explored nodes\n",
    "        nodes_explored += 1\n",
    "        \n",
    "        # Checks if the goal state has been reached\n",
    "        if np.array_equal(current_state, goal_state):\n",
    "            # Path reconstruction\n",
    "            path = []\n",
    "            while current_bytes in came_from:\n",
    "                prev_state, action = came_from[current_bytes]\n",
    "                path.append(current_state)\n",
    "                current_state = prev_state\n",
    "                current_bytes = current_state.tobytes()\n",
    "            path.reverse()\n",
    "            return path, len(path) - 1, nodes_explored\n",
    "        \n",
    "        explored.add(current_bytes)  # Marks the state as explored\n",
    "\n",
    "        # Expansion of the current node\n",
    "        for action in available_actions(current_state):\n",
    "            next_state = do_action(current_state, action)\n",
    "            next_bytes = next_state.tobytes()\n",
    "            \n",
    "            # If a node has already been expanded, there is no point in adding it to the frontier or updating its cumulative cost.\n",
    "            if next_bytes in explored:\n",
    "                continue\n",
    "            \n",
    "            tentative_g = g_costs[current_bytes] + 1  # Each move costs 1\n",
    "            \n",
    "            # The check ensures that the state next_bytes has never been added to the frontier.\n",
    "            # The check tentative_g < g_costs[next_bytes] ensures that the state next_bytes\n",
    "            # has already been added to the frontier but its cumulative cost can be updated\n",
    "            # because a path has been found that allows reaching it with a lower cost.\n",
    "            if next_bytes not in g_costs or tentative_g < g_costs[next_bytes]:\n",
    "                g_costs[next_bytes] = tentative_g\n",
    "                f_cost = tentative_g + manhattan_distance(next_state, goal_state)\n",
    "                heapq.heappush(frontier, (f_cost, next_bytes, next_state))\n",
    "                came_from[next_bytes] = (current_state, action)\n",
    "    \n",
    "    return [], 0, nodes_explored  # Returns an empty list if there is no solution\n",
    "\n",
    "# Start measuring the execution time of the algorithm\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Solve the puzzle\n",
    "solution_path, num_moves, nodes_explored = a_star_search_with_metrics(state, goal_state)\n",
    "\n",
    "# End of measuring the execution time of the algorithm\n",
    "end_time = datetime.now()\n",
    "\n",
    "# Show the stats\n",
    "\"\"\"print(\"Path:\")\n",
    "for step, state in enumerate(solution_path):\n",
    "    print(f\"Step {step}:\\n{state}\\n\")\"\"\"\n",
    "\n",
    "print(f\"Number of moves to solve the puzzle: {num_moves}\")\n",
    "print(f\"Number of explored nodes: {nodes_explored}\")\n",
    "# Calculate the execution time of the algorithm in the format hh:mm:ss.microsecond\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time}\")\n",
    "\n",
    "print(\"Starting state:\")\n",
    "print(solution_path[0])\n",
    "print()\n",
    "print(\"Goal state:\")\n",
    "print(solution_path[num_moves])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
